{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": " CS5265-Assignment-1-Gouru.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyMWMWNcB4HBRW6V8q+JfDJZ",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MaheshGouru/CS5265-Assignment-1/blob/main/CS5265_Assignment_1_Gouru.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Background\n",
    "With the rise of GPUs accelerating workloads for notably Apple's Neural Engine, Electric Vehicles autonomous functionalities, and data-mining for medicine, generating a benchmark for which configurations would be sufficiently suited for a developer would enable affordability, accessibility, and scalability amongst heterogenous architectures. Hosted on the UCI Machine Learning Repository as released from Enrique G. Paredes and Rafael Ballester-Ripoll of University of Zurich, the SGEMM GPU kernel performance Data Set publishes \"running times for multiplying two 2048 x 2048 matrices using a GPU OpenCL SGEMM kernel with varying parameters (using the library 'CLTune')\". SGEMM is short for \"Single precision GEneral Matrix Multiply\" which addresses matrix-matrix products of float 32 bits.\n",
    "\n",
    "The dataset has 14 parameters with 10 are ordinal and 4 are categorical (binary).\n",
    "\n",
    "The experiment used a workstation running Ubuntu 16.04 Linux with an Intel Core i5 (3.5GHz), 16GB RAM, and a NVidia Geforce GTX 680 4GB GF580 GTX-1.5GB GPU. This configuration can be replicated and reproduced for the sake of the exercise. The'gemm_fast' kernel from the automatic OpenCL kernel tuning library \"CLTune\" was used to interface the scripts with the GPU drivers.\n",
    "\n",
    "# Project Description\n",
    "Extracted from the dataset, the attribute information is as provided:\n",
    "\n",
    "## Independent variables:\n",
    "1-2. MWG, NWG: per-matrix 2D tiling at workgroup level: {16, 32, 64, 128} (integer)\n",
    "\n",
    "3.&nbsp; KWG: inner dimension of 2D tiling at workgroup level: {16, 32} (integer)\n",
    "\n",
    "4-5. MDIMC, NDIMC: local workgroup size: {8, 16, 32} (integer)\n",
    "\n",
    "6-7. MDIMA, NDIMB: local memory shape: {8, 16, 32} (integer)\n",
    "\n",
    "8.&nbsp; KWI: kernel loop unrolling factor: {2, 8} (integer)\n",
    "\n",
    "9-10. VWM, VWN: per-matrix vector widths for loading and storing: {1, 2, 4, 8} (integer)\n",
    "\n",
    "11-12. STRM, STRN: enable stride for accessing off-chip memory within a single thread: {0, 1} (categorical)\n",
    "\n",
    "13-14. SA, SB: per-matrix manual caching of the 2D workgroup tile: {0, 1} (categorical)\n",
    "\n",
    "The purpose would be interact with a wizard to recommend the most cost-effective configuration, such as those used on various cloud platforms like AWS, GCP, and Azure.\n",
    "\n",
    "# Performance Metric(s)\n",
    "The output are logged in columns 15-18. These columns log the performance times in milliseconds for 4 independent runs using the same parameters. As noted, the range is between 13.25 and 3397.08.\n",
    "\n",
    "As cited, a proportion of this data set was used to compute a tensor train based predictive model and estimate the Sobol sensitivity indices of all the parameters which could then be replicated as well to go through the exercise of benchmarking architectures for industry applications such as provided by TrTransaction Processing Performance Council (TPC) for decision support.\n",
    "\n",
    "#References\n",
    "https://archive.ics.uci.edu/ml/datasets/SGEMM+GPU+kernel+performance\n",
    "https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html\n",
    "\n",
    "---"
   ],
   "metadata": {
    "id": "YQe7Mqbf0f2C"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clarification on Datasets needed for the Project Assignments\n",
    "Just to recap some important highlights of the datasets needed for the upcoming project assignments:\n",
    "\n",
    "- Size: at least 5000 datapoints (samples/rows/records), less than 100,000 datapoints are recommended\n",
    "\n",
    "- #&nbsp;of columns: at least 7 attributes/features/variables (columns) are needed \n",
    "\n",
    "- Types of columns: a good dataset will contain not only numerical columns but also categorical columns (e.g., gender, dates, or anything else that has a finite number of categories) \n",
    "\n",
    "Some resources for acquiring data: \n",
    "\n",
    "- https://www.kaggle.com/datasets\n",
    "\n",
    "- https://datasetsearch.research.google.com/ \n",
    "\n",
    "- https://archive.ics.uci.edu/ml/datasets.php \n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "tCYJBYtO5cWE"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yr8tRuljz3uL"
   },
   "outputs": [],
   "source": []
  }
 ]
}